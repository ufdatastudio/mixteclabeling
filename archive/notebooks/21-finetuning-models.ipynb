{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbe1a477-2eb1-4d21-af1f-438a34cc270f",
   "metadata": {},
   "source": [
    "# Fine Tuning notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bf2a21c-c281-4b6f-8a24-072296fe27dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/apps/pytorch/2.0.1/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#%pip install pandas numpy torcheval torch matplotlib tensorboard torchvision=0.14.1\n",
    "#%pip uninstall -y torchvision\n",
    "# %pip install torch==1.13.1+cu117 torchvision==0.14.1+cu117 torchtext==0.14.1 torchaudio==0.13.1 torchdata==0.5.1 --extra-index-url https://download.pytorch.org/whl/cu117\n",
    "# %pip install opencv-python\n",
    "\n",
    "import getpass\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import data\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torchvision.models import get_model, get_model_weights, get_weight, list_models\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import random_split, ConcatDataset\n",
    "\n",
    "# from transformers import ViTFeatureExtractor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccad8d9f-1356-4c2e-9555-65207fbf8c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper params\n",
    "batch_size = 120\n",
    "learning_rate = 0.001\n",
    "epochs = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3e72e2c-bd88-4fb6-9d2b-87ffc89c747c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/orange/ufdatastudios/christan/mixteclabeling/notebooks\n",
      "Using device: cuda\n",
      "\n",
      "\n",
      "NVIDIA A100-SXM4-80GB\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}\\n')\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e179a5a2-6ee3-46a8-b177-c83882ff90aa",
   "metadata": {},
   "source": [
    "### Define path to images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25ec8508-f89e-499d-967a-478b24aec3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "basepath = Path(f'/home/{getpass.getuser()}/toorange/alexwebber/mixteclabeling') # Base data directory\n",
    "path_v = basepath / 'data/labeled_figures/codex_vindobonensis/gender/'\n",
    "path_n = basepath / 'data/labeled_figures/codex_nuttall/gender/'\n",
    "path_s = basepath / 'data/labeled_figures/codex_selden/gender/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9546cc9d-99f9-4568-bb9e-7b976aaf1373",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define image transforms\n",
    "## List of transforms https://pytorch.org/vision/stable/auto_examples/plot_transforms.html\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     # AddRandomBlockNoise(),\n",
    "     transforms.Resize((227, 227), antialias=True),\n",
    "     # transforms.Grayscale(),\n",
    "     \n",
    "     #transforms.ColorJitter(contrast=0.5),\n",
    "     #transforms.RandomRotation(360),     # Maybe useful for standng and sitting\n",
    "     #transforms.RandomHorizontalFlip(50),\n",
    "     #transforms.RandomVerticalFlip(50)\n",
    "])\n",
    "\n",
    "# transform = transforms.Compose([transforms.Resize((224, 224)), \n",
    "#                                 transforms.ToTensor(),\n",
    "#                                 transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "\n",
    "## Load images into PyTorch dataset\n",
    "vindobonensis_dataset = datasets.ImageFolder(path_v, transform=transform)\n",
    "nuttall_dataset = datasets.ImageFolder(path_n, transform=transform)\n",
    "selden_dataset = datasets.ImageFolder(path_s, transform=transform)\n",
    "\n",
    "figures_dataset = ConcatDataset([vindobonensis_dataset, nuttall_dataset, selden_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7c3c8f0-61dc-433b-9f4e-89ae6f28026a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6-11-2023-18-46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-11 18:46:28.520203: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-11 18:46:29.937270: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "def printdate(dt=datetime.datetime.now()):\n",
    "    \"\"\"print a date and time string containing only numbers and dashes\"\"\"\n",
    "\n",
    "    # your code here\n",
    "    if dt.hour < 10:\n",
    "        hour = '0' + str(dt.hour)\n",
    "    else:\n",
    "        hour = str(dt.hour)\n",
    "\n",
    "    if dt.minute < 10:\n",
    "        minute = '0' + str(dt.minute)\n",
    "    else:\n",
    "        minute = str(dt.minute)\n",
    "\n",
    "    d = '{}-{}-{}-{}-{}'.format(str(dt.month),str(dt.day),str(dt.year),hour,minute)\n",
    "    print(d)\n",
    "    return d\n",
    "    \n",
    "writer = SummaryWriter(log_dir=f'tmp/{printdate()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea5ec34f-c8ec-467b-9543-eb54b960d81b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "412, 137, 137\n"
     ]
    }
   ],
   "source": [
    "train_set, validation_set, test_set = random_split(figures_dataset, [0.6, 0.2, 0.2])\n",
    "\n",
    "print(f\"{len(train_set)}, {len(validation_set)}, {len(test_set)}\")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "validate_loader = torch.utils.data.DataLoader(validation_set, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size = batch_size,  shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46c21cd1-870f-4670-849e-27788ffa0bb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (17): ReLU(inplace=True)\n",
       "    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (24): ReLU(inplace=True)\n",
       "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (26): ReLU(inplace=True)\n",
       "    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (31): ReLU(inplace=True)\n",
       "    (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (33): ReLU(inplace=True)\n",
       "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (35): ReLU(inplace=True)\n",
       "    (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model = models.vgg16(pretrained=True)\n",
    "model = get_model(\"vgg19\", weights=torchvision.models.VGG19_Weights.DEFAULT)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d51d6f33-04d6-4e80-89b6-73b627e422eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze pretrained model parameters to avoid backpropogating through them\n",
    "for parameter in model.parameters():\n",
    "    parameter.requires_grad = False\n",
    "\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "# Build custom classifier\n",
    "classifier = nn.Sequential(OrderedDict([('fc1', nn.Linear(25088, 5000)),\n",
    "                                        ('relu', nn.ReLU()),\n",
    "                                        ('drop', nn.Dropout(p=0.5)),\n",
    "                                        ('fc2', nn.Linear(5000, 102)),\n",
    "                                        ('output', nn.LogSoftmax(dim=1))]))\n",
    "\n",
    "model.classifier = classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b4830ca-0a2a-462c-8c42-ae3f21b53a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for the validation pass\n",
    "def validation(model, validateloader, criterion):\n",
    "    \n",
    "    val_loss = 0\n",
    "    accuracy = 0\n",
    "    \n",
    "    for images, labels in iter(validateloader):\n",
    "\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        output = model.forward(images)\n",
    "        val_loss += criterion(output, labels).item()\n",
    "\n",
    "        probabilities = torch.exp(output)\n",
    "        \n",
    "        equality = (labels.data == probabilities.max(dim=1)[1])\n",
    "        accuracy += equality.type(torch.FloatTensor).mean()\n",
    "    \n",
    "    return val_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d06f682f-592f-41ab-bcc1-f9b21700c9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function and gradient descent\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "optimizer = optim.Adam(model.classifier.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45e0152c-0ac4-48ed-afd0-0fe5a2d2c3da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/15..  Train Loss: 0.312921..  Val Loss: 0.870329..  Val Accuracy: 0.900000\n",
      "Epoch: 3/15..  Train Loss: 0.302420..  Val Loss: 1.768469..  Val Accuracy: 0.887010\n",
      "Epoch: 4/15..  Train Loss: 0.219616..  Val Loss: 1.437138..  Val Accuracy: 0.832598\n",
      "Epoch: 5/15..  Train Loss: 0.052049..  Val Loss: 2.258097..  Val Accuracy: 0.882843\n",
      "Epoch: 7/15..  Train Loss: 0.000284..  Val Loss: 0.610623..  Val Accuracy: 0.933333\n",
      "Epoch: 8/15..  Train Loss: 0.000600..  Val Loss: 0.750802..  Val Accuracy: 0.924755\n",
      "Epoch: 9/15..  Train Loss: 0.000178..  Val Loss: 0.706828..  Val Accuracy: 0.962500\n",
      "Epoch: 10/15..  Train Loss: 0.010645..  Val Loss: 0.929518..  Val Accuracy: 0.907843\n",
      "Epoch: 12/15..  Train Loss: 0.001681..  Val Loss: 0.731031..  Val Accuracy: 0.962500\n",
      "Epoch: 13/15..  Train Loss: 0.000005..  Val Loss: 0.848013..  Val Accuracy: 0.903677\n",
      "Epoch: 14/15..  Train Loss: 0.000039..  Val Loss: 0.719313..  Val Accuracy: 0.937500\n",
      "Epoch: 15/15..  Train Loss: 0.000057..  Val Loss: 1.800655..  Val Accuracy: 0.928922\n"
     ]
    }
   ],
   "source": [
    "# Train the classifier\n",
    "\n",
    "def train_classifier():\n",
    "\n",
    "    steps = 0\n",
    "    print_every = 5\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    for e in range(epochs):\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        running_loss = 0\n",
    "\n",
    "        for images, labels in iter(train_loader):\n",
    "\n",
    "            steps += 1\n",
    "\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model.forward(images)\n",
    "            loss = criterion(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if steps % print_every == 0:\n",
    "\n",
    "                model.eval()\n",
    "\n",
    "                # Turn off gradients for validation, saves memory and computations\n",
    "                with torch.no_grad():\n",
    "                    validation_loss, accuracy = validation(model, validate_loader, criterion)\n",
    "\n",
    "                print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
    "                      \"Train Loss: {:.6f}.. \".format(running_loss/print_every),\n",
    "                      \"Val Loss: {:.6f}.. \".format(validation_loss/len(validate_loader)),\n",
    "                      \"Val Accuracy: {:.6f}\".format(accuracy/len(validate_loader)))\n",
    "\n",
    "\n",
    "                running_loss = 0\n",
    "                model.train()\n",
    "                    \n",
    "train_classifier()                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e1ca1fc-08b9-43ae-b50f-a485c7d78da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9624999761581421\n"
     ]
    }
   ],
   "source": [
    "def test_accuracy(model, test_loader):\n",
    "\n",
    "    # Do validation on the test set\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "    \n",
    "        accuracy = 0\n",
    "    \n",
    "        for images, labels in iter(test_loader):\n",
    "    \n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "    \n",
    "            output = model.forward(images)\n",
    "\n",
    "            probabilities = torch.exp(output)\n",
    "        \n",
    "            equality = (labels.data == probabilities.max(dim=1)[1])\n",
    "        \n",
    "            accuracy += equality.type(torch.FloatTensor).mean()\n",
    "        \n",
    "        print(\"Test Accuracy: {}\".format(accuracy/len(test_loader)))    \n",
    "        \n",
    "        \n",
    "test_accuracy(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e750d58d-22af-48a4-83b4-a386122df7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the checkpoint\n",
    "\n",
    "def save_checkpoint(model):\n",
    "\n",
    "    model.class_to_idx = {0: \"female\", 1: \"male\",\n",
    "                                        1.0: \"male\"}\n",
    "\n",
    "    checkpoint = {'arch': \"vgg16\",\n",
    "                  'class_to_idx': model.class_to_idx,\n",
    "                  'model_state_dict': model.state_dict()\n",
    "                 }\n",
    "\n",
    "    torch.save(checkpoint, 'checkpoint.pth')\n",
    "    \n",
    "save_checkpoint(model)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e4b69f1-f325-4b56-8d45-0e60a01e2a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "# Function that loads a checkpoint and rebuilds the model\n",
    "\n",
    "def load_checkpoint(filepath):\n",
    "    \n",
    "    checkpoint = torch.load(filepath)\n",
    "    \n",
    "    if checkpoint['arch'] == 'vgg16':\n",
    "        \n",
    "        model = models.vgg16(pretrained=True)\n",
    "        \n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "    else:\n",
    "        print(\"Architecture not recognized.\")\n",
    "    \n",
    "    model.class_to_idx = checkpoint['class_to_idx']\n",
    "    \n",
    "    classifier = nn.Sequential(OrderedDict([('fc1', nn.Linear(25088, 5000)),\n",
    "                                            ('relu', nn.ReLU()),\n",
    "                                            ('drop', nn.Dropout(p=0.5)),\n",
    "                                            ('fc2', nn.Linear(5000, 102)),\n",
    "                                            ('output', nn.LogSoftmax(dim=1))]))\n",
    "\n",
    "    model.classifier = classifier\n",
    "    \n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "#model = load_checkpoint('checkpoint.pth')\n",
    "#print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb0935ba-35be-4155-ad09-3d7115234d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(image_path):\n",
    "    ''' Scales, crops, and normalizes a PIL image for a PyTorch model,\n",
    "        returns an Numpy array\n",
    "    '''\n",
    "    \n",
    "    # Process a PIL image for use in a PyTorch model\n",
    "    \n",
    "    pil_image = Image.open(image_path)\n",
    "    \n",
    "    # Resize\n",
    "    if pil_image.size[0] > pil_image.size[1]:\n",
    "        pil_image.thumbnail((5000, 256))\n",
    "    else:\n",
    "        pil_image.thumbnail((256, 5000))\n",
    "        \n",
    "    # Crop \n",
    "    left_margin = (pil_image.width-224)/2\n",
    "    bottom_margin = (pil_image.height-224)/2\n",
    "    right_margin = left_margin + 224\n",
    "    top_margin = bottom_margin + 224\n",
    "    \n",
    "    pil_image = pil_image.crop((left_margin, bottom_margin, right_margin, top_margin))\n",
    "    \n",
    "    # Normalize\n",
    "    np_image = np.array(pil_image)/255\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    # np_image = (np_image - mean) / std\n",
    "    \n",
    "    # PyTorch expects the color channel to be the first dimension but it's the third dimension in the PIL image and Numpy array\n",
    "    # Color channel needs to be first; retain the order of the other two dimensions.\n",
    "    np_image = np_image.transpose((2, 0, 1))\n",
    "    \n",
    "    return np_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ea74c3ba-5847-4431-8a09-79342ddb25f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output.shape=torch.Size([120, 102])\n",
      "dict_items([(0, 'female'), (1, 'male')])\n",
      "{'female': 0, 'male': 1}\n",
      "[1.0, 0.0]\n",
      "[1.0, 6.728262391675344e-11]\n",
      "['female', 'male']\n"
     ]
    }
   ],
   "source": [
    "# Implement the code to predict the class from an image file\n",
    "\n",
    "def predict(image_path, model, topk=2):\n",
    "    ''' Predict the class (or classes) of an image using a trained deep learning model.\n",
    "    '''\n",
    "    \n",
    "    # image = process_image(image_path)\n",
    "    imageset = datasets.ImageFolder(image_path, transform=transform)\n",
    "    dataloader = torch.utils.data.DataLoader(imageset, batch_size = batch_size, shuffle = True)\n",
    "    \n",
    "#     # Convert image to PyTorch tensor first\n",
    "#     image = torch.from_numpy(image).type(torch.cuda.FloatTensor)\n",
    "#     print(image.shape)\n",
    "#     print(type(image))\n",
    "    \n",
    "#     # Returns a new tensor with a dimension of size one inserted at the specified position.\n",
    "#     image = image.unsqueeze(0)\n",
    "\n",
    "\n",
    "    data_iter = iter(test_loader)\n",
    "    # next(data_iter) # uncomment to get another test ietm\n",
    "    image, label = next(data_iter) #get the first test item\n",
    "    \n",
    "    output = model.forward(image.to(device))\n",
    "    print(f\"{output.shape=}\")\n",
    "    \n",
    "    probabilities = torch.exp(output)\n",
    "    \n",
    "    # Probabilities and the indices of those probabilities corresponding to the classes\n",
    "    top_probabilities, top_indices = probabilities.topk(topk)\n",
    "    \n",
    "    # Convert to lists\n",
    "    top_probabilities = top_probabilities.detach().type(torch.FloatTensor).numpy().tolist()[0] \n",
    "    top_indices = top_indices.detach().type(torch.FloatTensor).numpy().tolist()[0] \n",
    "    \n",
    "    # Convert topk_indices to the actual class labels using class_to_idx\n",
    "    # Invert the dictionary so you get a mapping from index to class.\n",
    "    \n",
    "    idx_to_class = {value: key for key, value in model.class_to_idx.items()}\n",
    "    print(model.class_to_idx.items())\n",
    "    print(idx_to_class)\n",
    "    print(top_indices)\n",
    "    \n",
    "    # top_classes = [idx_to_class[int(index)] for index in top_indices]\n",
    "    \n",
    "    # return top_probabilities, top_classes\n",
    "    return top_probabilities, ['female', 'male']\n",
    "    \n",
    "\n",
    "# probs, classes = predict('/orange/ufdatastudios/christan/mixteclabeling/044-a-03.png', model)\n",
    "probs, classes = predict('/orange/ufdatastudios/christan/mixteclabeling/notebooks/imgs', model)\n",
    "\n",
    "print(probs)\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8fb8d8b4-6a4c-41ea-8522-dafe0b832483",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<enum 'VGG19_Weights'>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from torchvision.models import get_model, get_model_weights, get_weight, list_models\n",
    "list_models()\n",
    "mod = torchvision.models.VGG19_Weights\n",
    "mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20a943d-49e0-44a8-bef6-5ceeacf93702",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch-2.0.1",
   "language": "python",
   "name": "pytorch-2.0.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
