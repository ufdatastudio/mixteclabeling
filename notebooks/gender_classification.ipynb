{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97408c26",
   "metadata": {},
   "source": [
    "# Mixtec Gender Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b1392b",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3812711c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/apps/pytorch/2.0.1/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2ab48ee578b0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %matplotlib inline\n",
    "# %pip install pandas numpy torcheval torch matplotlib tensorboard\n",
    "import os\n",
    "\n",
    "from pathlib import Path\n",
    "import PIL\n",
    "from PIL import Image\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import random_split, ConcatDataset\n",
    "from torchvision import datasets, transforms\n",
    "from torcheval.metrics import BinaryAccuracy, BinaryPrecision, BinaryRecall, BinaryF1Score\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa3c069",
   "metadata": {},
   "source": [
    "### Define hardware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "565069a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/orange/ufdatastudios/christan/mixteclabeling/notebooks\n",
      "Using device: cuda\n",
      "\n",
      "\n",
      "NVIDIA A100-SXM4-80GB\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%tensorboard` not found.\n"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}\\n')\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')\n",
    "\n",
    "%tensorboard --logdir=runs --bind_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52665bdc",
   "metadata": {},
   "source": [
    "### Define path to images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d884a966",
   "metadata": {},
   "outputs": [],
   "source": [
    "basepath = Path('/home/alexwebber/toorange/alexwebber/mixteclabeling') # Base data directory\n",
    "path_v = basepath / 'data/labeled_figures/codex_vindobonensis/gender/'\n",
    "path_n = basepath / 'data/labeled_figures/codex_nuttall/gender/'\n",
    "path_s = basepath / 'data/labeled_figures/codex_selden/gender/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aaa6e31",
   "metadata": {},
   "source": [
    "### Load figures into pandas, visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04e719c-6729-477a-808a-2983035c3517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Block Transform\n",
    "class AddRandomBlockNoise(torch.nn.Module):\n",
    "    def __init__(self, n_k=8, size=64):\n",
    "        super(AddRandomBlockNoise, self).__init__()\n",
    "        self.n_k = int(n_k * np.random.rand()) # Random number of boxes\n",
    "        self.size = int(size * np.random.rand()) # Max size\n",
    "    \n",
    "    def forward(self, tensor):\n",
    "        h, w = self.size, self.size\n",
    "        img = np.asarray(tensor)\n",
    "        img_size_x = img.shape[1]\n",
    "        img_size_y = img.shape[2]\n",
    "        boxes = []\n",
    "        for k in range(self.n_k):\n",
    "            if (img_size_y >= h or img_size_x >=w): break\n",
    "            print(f\"{h=} {w=} {img_size_x=} {img_size_y=}\")\n",
    "            x = np.random.randint(0, img_size_x-w, 1)[0] # FIXME the shape may be zero\n",
    "            y = np.random.randint(0, img_size_y-h, 1)[0]\n",
    "            img[:, y:y+h, x:x+w] = 0\n",
    "            boxes.append((x,y,h,w))\n",
    "        #img = Image.fromarray(img.astype('uint8'), 'RGB')\n",
    "        return torch.from_numpy(img)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(blocks={0}, size={1})'.format(self.n_k, self.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c45ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load CSV\n",
    "mixtec_figures = pd.read_csv(basepath / \"data/mixtec_figures.csv\")\n",
    "\n",
    "print(mixtec_figures.groupby('quality')['gender'].value_counts())\n",
    "print('\\n')\n",
    "print(mixtec_figures['gender'].value_counts())\n",
    "print('\\n')\n",
    "print(mixtec_figures['quality'].value_counts())\n",
    "\n",
    "## Load Tensorboard output\n",
    "writer = SummaryWriter(log_dir='runs/mixtec_experiment_gender')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce26638-a472-4e71-846e-1d39eb243c6f",
   "metadata": {},
   "source": [
    "### Load figures into datasets by codex, apply transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7fb441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Block Transform\n",
    "class AddRandomBlockNoise(torch.nn.Module):\n",
    "    def __init__(self, n_k=8, size=64):\n",
    "        super(AddRandomBlockNoise, self).__init__()\n",
    "        self.n_k = int(n_k * np.random.rand()) # Random number of boxes\n",
    "        self.size = int(size * np.random.rand()) # Max size\n",
    "    \n",
    "    def forward(self, tensor):\n",
    "        h, w = self.size, self.size\n",
    "        img = np.asarray(tensor)\n",
    "        img_size_x = img.shape[1]\n",
    "        img_size_y = img.shape[2]\n",
    "        boxes = []\n",
    "        for k in range(self.n_k):\n",
    "            if (img_size_y >= h or img_size_x >=w): break\n",
    "            print(f\"{h=} {w=} {img_size_x=} {img_size_y=}\")\n",
    "            x = np.random.randint(0, img_size_x-w, 1)[0] # FIXME the shape may be zero\n",
    "            y = np.random.randint(0, img_size_y-h, 1)[0]\n",
    "            img[:, y:y+h, x:x+w] = 0\n",
    "            boxes.append((x,y,h,w))\n",
    "        #img = Image.fromarray(img.astype('uint8'), 'RGB')\n",
    "        return torch.from_numpy(img)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(blocks={0}, size={1})'.format(self.n_k, self.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7f4fca-1cc7-48bd-be0a-543ec5f1f10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define image transforms\n",
    "## List of transforms https://pytorch.org/vision/stable/auto_examples/plot_transforms.html\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     AddRandomBlockNoise(),\n",
    "     transforms.Resize((227, 227), antialias=True),\n",
    "     # transforms.Grayscale(),\n",
    "     \n",
    "     #transforms.ColorJitter(contrast=0.5),\n",
    "     #transforms.RandomRotation(360),     # Maybe useful for standng and sitting\n",
    "     #transforms.RandomHorizontalFlip(50),\n",
    "     #transforms.RandomVerticalFlip(50)\n",
    "])\n",
    "\n",
    "## Load images into PyTorch dataset\n",
    "vindobonensis_dataset = datasets.ImageFolder(path_v, transform=transform)\n",
    "nuttall_dataset = datasets.ImageFolder(path_n, transform=transform)\n",
    "selden_dataset = datasets.ImageFolder(path_s, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e4528d-72a7-4744-8019-a155bdb29f7b",
   "metadata": {},
   "source": [
    "### Concatenate datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af14e6d2-6087-4b96-985b-75bebbda6a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "figures_dataset = ConcatDataset([vindobonensis_dataset, nuttall_dataset, selden_dataset])\n",
    "\n",
    "print(figures_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f72d2b8-b7d8-4d7b-af7e-6fc40fbaeac5",
   "metadata": {},
   "source": [
    "### Assign classes to map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ad6aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_map = {0: \"female\", 1: \"male\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13794896",
   "metadata": {},
   "source": [
    "### Print random image for sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921763f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access a random image from the dataset\n",
    "\n",
    "for i in range(1):\n",
    "    random_index = np.random.randint(len(figures_dataset))\n",
    "    image, label = figures_dataset[random_index]\n",
    "\n",
    "    # Convert the image tensor to a NumPy array and transpose it\n",
    "    image = image.permute(1, 2, 0)\n",
    "    image = image.numpy()\n",
    "\n",
    "    # Display the image\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4a14d2",
   "metadata": {},
   "source": [
    "### Visualize dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4922a7e-23f0-4202-8d1b-8728b3faf7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_classes(dataset, n_classes=2):\n",
    "    image_count = [0]*(n_classes)\n",
    "    for img in dataset:\n",
    "        image_count[img[1]] += 1\n",
    "    return image_count\n",
    "\n",
    "def sampler_(dataset, n_classes=2):\n",
    "    dataset_counts = count_classes(dataset)\n",
    "    num_samples = len(dataset_counts)\n",
    "    labels = [tag for _,tag in dataset]\n",
    "\n",
    "    class_weights = [num_samples/dataset_counts[i] for i in range(n_classes)]\n",
    "    weights = [class_weights[labels[i]] for i in range(num_samples)]\n",
    "    sampler = WeightedRandomSampler(torch.DoubleTensor(weights), int(num_samples), replacement=True)\n",
    "    return sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39c2c19-90fb-4c83-ba0c-c4a7665f52bc",
   "metadata": {},
   "source": [
    "### Split combined dataset into training and testing sets and load into DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445e34a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "train_set, test_set = random_split(figures_dataset, [0.8, 0.2])\n",
    "\n",
    "sampler = sampler_(train_set.dataset)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size = batch_size,  shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1176bbb4",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9216d29e",
   "metadata": {},
   "source": [
    "### Define CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77a3c00-2794-4cf9-8717-9a0101fc394f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool1 = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.conv2 = torch.nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool2 = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = torch.nn.Linear(16 * 56 * 56, 1568)  # Adjusted size\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.dropout1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool2(x)\n",
    "        x = x.view(-1, 16 * 56 * 56)\n",
    "        \n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc1(x)\n",
    "        x = x.view(batch_size, -1)\n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b75b53-902e-4196-9ec7-09e2f77351ba",
   "metadata": {},
   "source": [
    "### Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2785ef-8e8a-4c86-a24f-43100c545534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "cnn = CNN()\n",
    "cnn.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc88951",
   "metadata": {},
   "source": [
    "### Define hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c69ed0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lossweight = torch.tensor([1.5,1.0]).to(device)\n",
    "#lossweight.to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "#criterion = torch.nn.NLLLoss()\n",
    "#optimizer = torch.optim.SGD(cnn.parameters(), lr=0.01, momentum=0.9)\n",
    "optimizer = torch.optim.Adam(cnn.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039e5de8",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e6cdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(5):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    losses = []\n",
    "    \n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        \n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        # forward\n",
    "        outputs = cnn(inputs)\n",
    "        \n",
    "        # metrics\n",
    "        train_loss = criterion(outputs, labels)\n",
    "        #train_accuracy = torch.sum(outputs == labels)\n",
    "        running_loss += train_loss.item()\n",
    "        \n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        \n",
    "        # gradient descent or adam step\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        \n",
    "        if i % batch_size == 0:\n",
    "            print(\"Epoch: \" + str(epoch + 1) + \" | \" \"Loss: \" + str(running_loss))\n",
    "            \n",
    "            # write to TensorBoard\n",
    "            writer.add_scalar('Loss/train', losses[n_iter], n_iter)\n",
    "        \n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5591bd4-2b36-48cb-9092-bacdf6e3514e",
   "metadata": {},
   "source": [
    "### Write to TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee516edd-6fbf-4a30-a244-f05134bdc44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe10c1b",
   "metadata": {},
   "source": [
    "### View incorrectly labeled samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880d1a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, data in enumerate(train_loader, 0):\n",
    "    images, labels = data[0].to(device), data[1].to(device)\n",
    "    \n",
    "    outputs = cnn(images)\n",
    "    \n",
    "    _, predictions = torch.max(outputs, 1)\n",
    "    \n",
    "    correct = 0\n",
    "    total = len(predictions)\n",
    "#     for label, image, prediction in zip(labels, images, predictions):\n",
    "#         if label != prediction:\n",
    "#             image = image.permute(1, 2, 0)\n",
    "#             image = image.cpu().numpy()\n",
    "\n",
    "#             plt.imshow(image)\n",
    "#             plt.title(\"Prediction: \" + class_map[prediction.item()] + \" | Label: \" + class_map[label.item()])\n",
    "#             plt.axis('off')\n",
    "#             plt.show()\n",
    "            \n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b816c50",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa40463",
   "metadata": {},
   "outputs": [],
   "source": [
    "savepath = \"../models/mixtec_gender_classifier.pth\"\n",
    "\n",
    "torch.save(cnn.state_dict(), savepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16111667",
   "metadata": {},
   "source": [
    "## Visualize learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1579a43e",
   "metadata": {},
   "source": [
    "### Define tensorboard output functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad9e080",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "def matplotlib_imshow(img, one_channel=False):\n",
    "    if one_channel:\n",
    "        img = img.mean(dim=0)\n",
    "    img = img / 2 + 0.5\n",
    "    npimg = img.numpy()\n",
    "    if one_channel:\n",
    "        plt.imshow(npimg, cmap=\"Greys\")\n",
    "    else:\n",
    "        plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "        \n",
    "def gen_plot(img):\n",
    "    img = img.mean(dim=0)\n",
    "    img = img / 2 + 0.5\n",
    "    npimg = img.numpy()\n",
    "    \n",
    "    \n",
    "    plt.figure()\n",
    "    plt.imshow(npimg, cmap=\"inferno\")\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format='jpeg')\n",
    "    buf.seek(0)\n",
    "    \n",
    "    return buf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a6a859",
   "metadata": {},
   "source": [
    "### Output sample heatmap of selected features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0159956",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(train_loader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "image_grid = torchvision.utils.make_grid(images)\n",
    "\n",
    "# Prepare the plot\n",
    "plot_buf = gen_plot(image_grid)\n",
    "\n",
    "image = Image.open(plot_buf)\n",
    "image = transforms.ToTensor()(image).unsqueeze(0)\n",
    "\n",
    "#img_grid = torchvision.utils.make_grid(images)\n",
    "\n",
    "# show images\n",
    "# matplotlib_imshow(image, one_channel=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa58d240",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2a4af0",
   "metadata": {},
   "source": [
    "### Load images and labels from test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079343e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iter = iter(test_loader)\n",
    "images, labels = next(data_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24bb8321",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90c1fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = CNN()\n",
    "cnn.load_state_dict(torch.load(savepath))\n",
    "cnn.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f0ab5d",
   "metadata": {},
   "source": [
    "### Produce predictions and calculate accuracy of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb3fe96",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.eval()\n",
    "\n",
    "predicted_list = []\n",
    "target_list = []\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        images, labels = data[0].to(device), data[1].to(device)\n",
    "        target_list += labels.cpu()\n",
    "        \n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = cnn(images)\n",
    "        \n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        print(_)\n",
    "        predicted_list += predicted\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the {str(len(test_set))} test images: {100 * correct // total} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb6ef1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Predicted: {torch.tensor(predicted_list)}\")\n",
    "print(f\"Truth    : {torch.tensor(target_list)}\")\n",
    "\n",
    "metric_names = [\"Accuracy\", \"Precision\", \"Recall\", \"F1\"]\n",
    "metrics = [BinaryAccuracy(), BinaryPrecision(), BinaryRecall(), BinaryF1Score()]\n",
    "\n",
    "for metric, name in zip(metrics, metric_names):\n",
    "    metric.update(torch.tensor(predicted_list), torch.tensor(target_list))\n",
    "    print(f\"{name:<9}: {metric.compute()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch-2.0.1",
   "language": "python",
   "name": "pytorch-2.0.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
